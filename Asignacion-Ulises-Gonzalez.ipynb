{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "Inici\u00e9 mi entorno de an\u00e1lisis utilizando Apache Spark. Primero, import\u00e9 las librer\u00edas necesarias \u2014en este caso, SparkSession y DataFrame\u2014 que son fundamentales para trabajar con datos en Spark.\n\nDespu\u00e9s, cre\u00e9 una instancia de SparkSession, que es el punto de entrada principal para trabajar con DataFrames y ejecutar consultas en Spark. Le asign\u00e9 un nombre a la aplicaci\u00f3n: \"WorldBank Education Analysis\", para mantener organizado mi entorno y facilitar la identificaci\u00f3n del proceso si necesito monitorearlo.\n\nUna vez creado el SparkSession, le\u00ed los archivos de datos en formato Parquet que se encontraban en una ruta de Google Cloud Storage (gs://scala-spark-datos/education_data/*.parquet). Esta lectura gener\u00f3 un DataFrame llamado worldBankDF, que contiene toda la informaci\u00f3n educativa del Banco Mundial que voy a analizar.\n\nPara entender la estructura del conjunto de datos, utilic\u00e9 el m\u00e9todo .printSchema() que me permiti\u00f3 ver las columnas disponibles y sus tipos de datos. Finalmente, visualic\u00e9 las primeras 5 filas con .show(5), lo cual me ayud\u00f3 a hacer una inspecci\u00f3n inicial de los datos y confirmar que todo se hubiera cargado correctamente."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- country_name: string (nullable = true)\n |-- country_code: string (nullable = true)\n |-- indicator_name: string (nullable = true)\n |-- indicator_code: string (nullable = true)\n |-- value: double (nullable = true)\n |-- year: long (nullable = true)\n\n+------------+------------+--------------------+--------------+--------+----+\n|country_name|country_code|      indicator_name|indicator_code|   value|year|\n+------------+------------+--------------------+--------------+--------+----+\n|        Chad|         TCD|Enrolment in lowe...|       UIS.E.2|321921.0|2012|\n|        Chad|         TCD|Enrolment in uppe...|       UIS.E.3| 68809.0|2006|\n|        Chad|         TCD|Enrolment in uppe...|       UIS.E.3| 30551.0|1999|\n|        Chad|         TCD|Enrolment in uppe...|       UIS.E.3| 79784.0|2007|\n|        Chad|         TCD|Repeaters in prim...|       UIS.R.1|282699.0|2006|\n+------------+------------+--------------------+--------------+--------+----+\nonly showing top 5 rows\n\n"}, {"data": {"text/plain": "import org.apache.spark.sql.{SparkSession, DataFrame}\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2bf433eb\nworldBankDF: org.apache.spark.sql.DataFrame = [country_name: string, country_code: string ... 4 more fields]\n"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "import org.apache.spark.sql.{SparkSession, DataFrame}\nval spark = SparkSession.builder()\n  .appName(\"WorldBank Education Analysis\")\n  .getOrCreate()\nval worldBankDF = spark.read\n  .parquet(\"gs://scala-spark-datos/education_data/*.parquet\")\nworldBankDF.printSchema()\nworldBankDF.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "topCountries: Array[org.apache.spark.sql.Row] = Array()\n"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "En este ejercicio comenc\u00e9 explorando el conjunto de datos del Banco Mundial para entender qu\u00e9 informaci\u00f3n ten\u00eda disponible. Primero, consult\u00e9 los a\u00f1os \u00fanicos presentes en el dataset para saber qu\u00e9 per\u00edodos pod\u00eda analizar. Luego hice lo mismo con los c\u00f3digos de indicadores, ya que me interesaba enfocar el an\u00e1lisis en uno espec\u00edfico: la tasa de matriculaci\u00f3n primaria, cuyo c\u00f3digo es SE.PRM.ENRR.\n\nUna vez identificado el indicador de inter\u00e9s, filtr\u00e9 los datos para quedarme solo con ese y luego agrup\u00e9 por a\u00f1o para contar cu\u00e1ntos registros ten\u00eda por cada uno. Orden\u00e9 los resultados de forma descendente para que el a\u00f1o m\u00e1s reciente apareciera de primero. As\u00ed pude identificar autom\u00e1ticamente cu\u00e1l era el a\u00f1o m\u00e1s reciente con datos disponibles, y lo guard\u00e9 en una variable llamada latestYear.\n\nCon esa informaci\u00f3n, modifiqu\u00e9 mi consulta original \u2014que usaba un a\u00f1o fijo (2015)\u2014 para que tomara din\u00e1micamente el a\u00f1o m\u00e1s reciente. Luego filtr\u00e9 el dataset usando ese a\u00f1o y el indicador de matriculaci\u00f3n primaria, orden\u00e9 los pa\u00edses por el valor de la tasa en orden descendente, seleccion\u00e9 los 10 primeros, y extraje sus nombres y valores."}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+\n|year|\n+----+\n|1970|\n|1971|\n|1972|\n|1973|\n|1974|\n|1975|\n|1976|\n|1977|\n|1978|\n|1979|\n|1980|\n|1981|\n|1982|\n|1983|\n|1984|\n|1985|\n|1986|\n|1987|\n|1988|\n|1989|\n|1990|\n|1991|\n|1992|\n|1993|\n|1994|\n|1995|\n|1996|\n|1997|\n|1998|\n|1999|\n|2000|\n|2001|\n|2002|\n|2003|\n|2004|\n|2005|\n|2006|\n|2007|\n|2008|\n|2009|\n|2010|\n|2011|\n|2012|\n|2013|\n|2014|\n|2015|\n|2016|\n+----+\n\n+--------------+\n|indicator_code|\n+--------------+\n|   SE.COM.DURS|\n|    UIS.E.1.G2|\n|  UIS.NIRA.1.M|\n| UIS.LR.AG65.M|\n|  UIS.REPR.1.M|\n|  UIS.UAEP.1.F|\n| UIS.ROFST.2.M|\n|   UIS.GOER.56|\n|  UIS.R.1.G4.F|\n|  UIS.UAEP.1.M|\n| HH.DHS.OOST.X|\n| UIS.E.3.GPV.F|\n|    UIS.E.1.PR|\n|   UIS.THDUR.0|\n|  UIS.SLE.12.M|\n|SE.PRM.ENRL.FE|\n|SE.TER.ENRR.FE|\n|   UIS.LR.AG65|\n|  UIS.NERA.2.F|\n|   UIS.PTRHC.3|\n|SL.TLF.TOTL.IN|\n| UIS.REPP.1.G6|\n|   UIS.NE.1.G1|\n|  UIS.TRTP.0.F|\n| UIS.ROFST.1.M|\n|SL.UEM.TOTL.ZS|\n|    UIS.SLE.56|\n|   UIS.SLE.4.F|\n|   UIS.XPUBP.3|\n|SE.PRE.ENRR.MA|\n|   UIS.DR.1.G5|\n|HH.MICS.SCR.Q2|\n|SE.SEC.PRIV.ZS|\n| UIS.REPR.1.G4|\n|HH.DHS.OOS.1.U|\n|SE.PRE.ENRL.FE|\n|SE.SEC.ENRR.FE|\n|    UIS.SLE.23|\n|   SE.PRM.TENR|\n|   SE.SEC.ENRL|\n|    UIS.E.1.G5|\n|   SE.PRM.AGES|\n|NY.GNP.PCAP.CD|\n|SE.PRM.TENR.FE|\n|SE.SEC.ENRR.LO|\n|     UIS.G.8.T|\n|   UIS.ESL.1.M|\n| UIS.REPP.1.G5|\n|   UIS.E.02.PR|\n|  UIS.GER.12.F|\n| UIS.DR.1.G1.M|\n|HH.DHS.NIR.1.F|\n|   SE.PRM.ENRR|\n|  UIS.OFST.2.F|\n|SE.PRM.ENRR.FE|\n|     UIS.FEP.0|\n| UIS.NART.2.Q2|\n|SE.PRM.ENRR.MA|\n|  UIS.R.1.G6.F|\n|SE.XPD.TERT.ZS|\n|  UIS.E.1.G5.F|\n| UIS.REPR.1.G1|\n|    UIS.E.1.G1|\n|  UIS.NERA.2.M|\n|  UIS.SLE.56.M|\n|NY.GNP.MKTP.CD|\n|    UIS.TRTP.0|\n|   SH.DYN.MORT|\n| UIS.NE.1.G1.F|\n|   UIS.THAGE.0|\n|  UIS.SLE.23.M|\n|SE.SEC.ENRL.GC|\n|   UIS.XPUBP.2|\n|  UIS.E.0.PR.T|\n|   SE.PRM.ENRL|\n|  UIS.FOFSTP.1|\n|    UIS.R.1.G5|\n|  UIS.E.3.PU.F|\n|HH.DHS.NAR.1.U|\n|    UIS.REPR.1|\n| UIS.DR.1.G3.F|\n|   SE.TER.TCHR|\n|SE.SEC.ENRR.MA|\n|    UIS.E.4.PR|\n|    UIS.NERT.2|\n|   UIS.DR.1.G3|\n| UIS.FEP.3.GPV|\n|   UIS.DR.1.G4|\n| UIS.DR.1.G2.F|\n|   UIS.MS.56.T|\n|   UIS.ROFST.2|\n|SE.SEC.PROG.ZS|\n|    UIS.E.2.PR|\n|  UIS.R.1.G2.F|\n| UIS.E.23.PR.F|\n|SE.PRM.NINT.ZS|\n|   UIS.ESL.1.T|\n|   UIS.SAP.4.F|\n|  UIS.T.23.V.F|\n|NY.GDP.PCAP.KD|\n+--------------+\nonly showing top 100 rows\n\n+----+-----+\n|year|count|\n+----+-----+\n|2010|    1|\n|2005|    1|\n|2003|    1|\n|1999|    1|\n|1998|    1|\n|1994|    1|\n|1988|    1|\n+----+-----+\n\nEl a\u00f1o m\u00e1s reciente disponible es: 2010\nTop 10 pa\u00edses por tasa de matriculaci\u00f3n primaria (2010):\nChad                   82.61 ********\n"}, {"data": {"text/plain": "availableData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [year: bigint, count: bigint]\nlatestYear: Long = 2010\ntopCountriesUpdated: Array[org.apache.spark.sql.Row] = Array([Chad,82.6077194213867])\n"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "worldBankDF.select(\"year\").distinct().orderBy(\"year\").show(100)\n\nworldBankDF.select(\"indicator_code\").distinct().show(100)\n\nval availableData = worldBankDF\n  .filter($\"indicator_code\" === \"SE.PRM.ENRR\")\n  .groupBy(\"year\")\n  .count()\n  .orderBy(desc(\"year\"))\n\navailableData.show()\n\n\nval latestYear = availableData.first().getAs[Long](\"year\")\nprintln(s\"El a\u00f1o m\u00e1s reciente disponible es: $latestYear\")\n\n\nval topCountriesUpdated = worldBankDF\n  .filter($\"indicator_code\" === \"SE.PRM.ENRR\" && $\"year\" === latestYear)\n  .orderBy(desc(\"value\"))\n  .limit(10)\n  .select(\"country_name\", \"value\")\n  .collect()\n\nprintln(s\"Top 10 pa\u00edses por tasa de matriculaci\u00f3n primaria ($latestYear):\")\ntopCountriesUpdated.foreach { row =>\n  val country = row.getAs[String](\"country_name\")\n  val value = row.getAs[Double](\"value\")\n  val stars = \"*\" * (value / 10).toInt\n  println(f\"$country%-20s $value%7.2f $stars\")\n}"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "Ejercicio 1 adaptado: Trabajar con el DataFrame de educaci\u00f3n del Banco Mundial\n1. Mostrar el esquema del DataFrame (equivale a mostrar el esquema de estudiantes)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Esquema del DataFrame:\nroot\n |-- country_name: string (nullable = true)\n |-- country_code: string (nullable = true)\n |-- indicator_name: string (nullable = true)\n |-- indicator_code: string (nullable = true)\n |-- value: double (nullable = true)\n |-- year: long (nullable = true)\n\n"}], "source": "println(\"Esquema del DataFrame:\")\nworldBankDF.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "2. Filtrar registros con un valor mayor a 80 (equivalente a filtrar estudiantes con calificaci\u00f3n > 8, adaptado a escala 0-100)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Registros con valor > 80:\n+------------+------------+--------------------+--------------+--------+----+\n|country_name|country_code|      indicator_name|indicator_code|   value|year|\n+------------+------------+--------------------+--------------+--------+----+\n|        Chad|         TCD|Enrolment in lowe...|       UIS.E.2|321921.0|2012|\n|        Chad|         TCD|Enrolment in uppe...|       UIS.E.3| 68809.0|2006|\n|        Chad|         TCD|Enrolment in uppe...|       UIS.E.3| 30551.0|1999|\n|        Chad|         TCD|Enrolment in uppe...|       UIS.E.3| 79784.0|2007|\n|        Chad|         TCD|Repeaters in prim...|       UIS.R.1|282699.0|2006|\n|        Chad|         TCD|Repeaters in prim...|       UIS.R.1|169600.0|1991|\n|        Chad|         TCD|Repeaters in prim...|       UIS.R.1| 79342.0|1977|\n|        Chad|         TCD|Repeaters in prim...|       UIS.R.1|251163.0|2001|\n|        Chad|         TCD|Repeaters in prim...|       UIS.R.1|224397.0|2000|\n|        Chad|         TCD|Teachers in lower...|       UIS.T.2|  2703.0|2000|\n+------------+------------+--------------------+--------------+--------+----+\nonly showing top 10 rows\n\n"}, {"data": {"text/plain": "altosValoresDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country_name: string, country_code: string ... 4 more fields]\n"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "val altosValoresDF = worldBankDF.filter($\"value\" > 80)\nprintln(\"Registros con valor > 80:\")\naltosValoresDF.show(10)"}, {"cell_type": "markdown", "metadata": {}, "source": "3. Seleccionar nombres de pa\u00edses y ordenarlos por valor (calificaci\u00f3n) de forma descendente\n(equivalente a ordenar nombres de estudiantes por calificaci\u00f3n)"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Pa\u00edses ordenados por valor (descendente):\n+------------+--------------------+-------------------+----+\n|country_name|      indicator_name|              value|year|\n+------------+--------------------+-------------------+----+\n|        Chad|GDP at market pri...|1.23680710387362E10|2012|\n|        Chad|GDP at market pri...|1.21563804250825E10|2011|\n|        Chad|GDP at market pri...| 1.0666537555594E10|2011|\n|        Chad|GDP at market pri...|1.03519326044154E10|2008|\n|        Chad|   GNI (current US$)|1.03023488371154E10|2010|\n|        Chad|GDP at market pri...| 9.00605773352815E9|2008|\n|        Chad|GDP at market pri...| 8.40792103468586E9|2005|\n|        Chad|GDP at market pri...| 3.21482886992383E9|1989|\n|        Chad|GDP at market pri...| 2.28465807624205E9|1972|\n|        Chad|GDP at market pri...|  2.2808611566978E9|1983|\n+------------+--------------------+-------------------+----+\nonly showing top 10 rows\n\n"}, {"data": {"text/plain": "paisesPorValor: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country_name: string, indicator_name: string ... 2 more fields]\n"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "val paisesPorValor = worldBankDF\n  .select(\"country_name\", \"indicator_name\", \"value\", \"year\")\n  .orderBy(desc(\"value\")) \nprintln(\"Pa\u00edses ordenados por valor (descendente):\")\npaisesPorValor.show(10)"}, {"cell_type": "markdown", "metadata": {}, "source": "Ejercicio 2: UDF (User Defined Function)\n\nPregunta: Define una funci\u00f3n que determine si un n\u00famero es par o impar. Aplica esta funci\u00f3n a una columna de un DataFrame que contenga una lista de n\u00fameros.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "En este ejercicio trabaj\u00e9 con una funci\u00f3n definida por m\u00ed (una UDF) para evaluar valores educativos en relaci\u00f3n con un umbral que establec\u00ed previamente. El objetivo fue clasificar cada valor como \"por encima\" o \"por debajo\" del umbral, facilitando as\u00ed el an\u00e1lisis de los datos.\n\nPrimero cre\u00e9 una funci\u00f3n personalizada que toma un valor educativo como entrada y lo compara con un umbral. Seg\u00fan el resultado de esa comparaci\u00f3n, la funci\u00f3n devuelve una etiqueta indicando si el valor est\u00e1 por encima o por debajo del umbral. Luego apliqu\u00e9 esta funci\u00f3n a la columna value de mi DataFrame. Esto me permiti\u00f3 evaluar cada fila del conjunto de datos de manera individual, asignando la categor\u00eda correspondiente seg\u00fan el resultado de la comparaci\u00f3n.\nLuego podemos mirar el DataFrame resultante, que ahora incluye una nueva columna con la clasificaci\u00f3n generada por la UDF. "}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+--------------------+--------+-----------+----+\n|country_name|      indicator_name|   value|nivel_valor|year|\n+------------+--------------------+--------+-----------+----+\n|        Chad|Enrolment in lowe...|321921.0|       Alto|2012|\n|        Chad|Enrolment in uppe...| 68809.0|       Alto|2006|\n|        Chad|Enrolment in uppe...| 30551.0|       Bajo|1999|\n|        Chad|Enrolment in uppe...| 79784.0|       Alto|2007|\n|        Chad|Repeaters in prim...|282699.0|       Alto|2006|\n|        Chad|Repeaters in prim...|169600.0|       Alto|1991|\n|        Chad|Repeaters in prim...| 79342.0|       Alto|1977|\n|        Chad|Repeaters in prim...|251163.0|       Alto|2001|\n|        Chad|Repeaters in prim...|224397.0|       Alto|2000|\n|        Chad|Teachers in lower...|  2703.0|       Bajo|2000|\n+------------+--------------------+--------+-----------+----+\nonly showing top 10 rows\n\n"}, {"data": {"text/plain": "import org.apache.spark.sql.functions.{udf, col}\nevaluateValueUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction($Lambda$3876/1787500908@4a507d4d,StringType,Some(List(DoubleType)))\nresultDF: org.apache.spark.sql.DataFrame = [country_name: string, country_code: string ... 5 more fields]\n"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "import org.apache.spark.sql.functions.{udf, col}\nval evaluateValueUDF = udf((value: Double) => {if (value > 50000.0) \"Alto\" else \"Bajo\"})\nval resultDF = worldBankDF.withColumn(\"nivel_valor\", evaluateValueUDF(col(\"value\")))\nresultDF.select(\"country_name\", \"indicator_name\", \"value\", \"nivel_valor\", \"year\").show(10)"}, {"cell_type": "markdown", "metadata": {}, "source": "Ejercicio 3: Joins y agregaciones\n  Pregunta: Dado dos DataFrames,\n            uno con informaci\u00f3n de estudiantes (id, nombre)\n            y otro con calificaciones (id_estudiante, asignatura, calificacion),\n            realiza un join entre ellos y calcula el promedio de calificaciones por estudiante."}, {"cell_type": "markdown", "metadata": {}, "source": "En este ejercicio adapt\u00e9 la l\u00f3gica original \u2014que consist\u00eda en unir un DataFrame de estudiantes con otro de calificaciones para calcular promedios por estudiante\u2014 al contexto del dataset educativo del Banco Mundial que ya ten\u00eda cargado. En lugar de estudiantes y calificaciones, trabaj\u00e9 con pa\u00edses e indicadores educativos.\n\nPrimero, cre\u00e9 un DataFrame llamado countriesDF que contiene informaci\u00f3n \u00fanica por pa\u00eds, espec\u00edficamente el nombre y el c\u00f3digo del pa\u00eds. Esto lo hice a partir del worldBankDF, seleccionando las columnas country_name y country_code y eliminando duplicados con .distinct(). Este paso simula el DataFrame de estudiantes del ejercicio original, solo que aqu\u00ed los \u201cestudiantes\u201d son pa\u00edses.\n\nLuego, prepar\u00e9 otro DataFrame llamado indicatorsDF, donde trabaj\u00e9 con los indicadores educativos como si fueran \u201ccalificaciones\u201d. Seleccion\u00e9 el c\u00f3digo del pa\u00eds, el nombre del indicador, su valor y el a\u00f1o, y filtr\u00e9 aquellos registros en los que el valor no fuera nulo. Esto representa las \u201cnotas\u201d que cada pa\u00eds ha recibido en distintos indicadores a lo largo del tiempo.\n\nA continuaci\u00f3n, realic\u00e9 un join interno entre los dos DataFrames usando el c\u00f3digo del pa\u00eds como clave de uni\u00f3n. Esto me permiti\u00f3 combinar la informaci\u00f3n general de los pa\u00edses con sus respectivos indicadores educativos en un solo DataFrame (joinedDF), de manera similar a c\u00f3mo en el ejercicio original se un\u00edan estudiantes con sus calificaciones.\n\nDespu\u00e9s de tener los datos combinados, calcul\u00e9 el promedio de los indicadores por pa\u00eds. Para ello, agrup\u00e9 por country_name y country_code, y apliqu\u00e9 funciones de agregaci\u00f3n: saqu\u00e9 el promedio de los valores (avg) y cont\u00e9 cu\u00e1ntos indicadores ten\u00eda cada pa\u00eds. Orden\u00e9 los resultados de forma descendente para ver qu\u00e9 pa\u00edses ten\u00edan, en promedio, mejores valores educativos."}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+------------+--------------------+-----------------+\n|country_name|country_code|promedio_indicadores|total_indicadores|\n+------------+------------+--------------------+-----------------+\n|        Chad|         TCD| 9.660887661559488E7|             1000|\n+------------+------------+--------------------+-----------------+\n\n"}, {"data": {"text/plain": "countriesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country_name: string, country_code: string]\nindicatorsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country_code: string, indicator_name: string ... 2 more fields]\njoinedDF: org.apache.spark.sql.DataFrame = [country_code: string, country_name: string ... 3 more fields]\navgByCountryDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country_name: string, country_code: string ... 2 more fields]\n"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "val countriesDF = worldBankDF\n  .select(\"country_name\", \"country_code\") //DataFrame con informaci\u00f3n resumida por pa\u00eds\n  .distinct()\n\nval indicatorsDF = worldBankDF\n  .select(\"country_code\", \"indicator_name\", \"value\", \"year\") // indicadores educativos como si fueran \"calificaciones\"\n  .filter(col(\"value\").isNotNull)\n\nval joinedDF = countriesDF.join(indicatorsDF, Seq(\"country_code\"), \"inner\")\n\nval avgByCountryDF = joinedDF\n  .groupBy(\"country_name\", \"country_code\")\n  .agg(\n    avg(\"value\").alias(\"promedio_indicadores\"),\n    count(\"indicator_name\").alias(\"total_indicadores\")\n  )\n  .orderBy(desc(\"promedio_indicadores\"))\n\navgByCountryDF.show(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "Ejercicio 4: Uso de RDDs\nPregunta: Crea un RDD a partir de una lista de palabras y cuenta la cantidad de ocurrencias de cada palabra."}, {"cell_type": "markdown", "metadata": {}, "source": "Primero, tom\u00e9 el DataFrame original (worldBankDF) y seleccion\u00e9 \u00fanicamente la columna indicator_name, que contiene el nombre de cada indicador educativo. Luego lo convert\u00ed a un RDD utilizando .rdd y lo transform\u00e9 en una lista de strings con .map, extrayendo cada nombre de indicador. As\u00ed obtuve un RDD plano con todos los nombres de indicadores presentes en el conjunto de datos.\n\nCon este RDD, apliqu\u00e9 un conteo de frecuencia para saber cu\u00e1ntas veces aparece cada indicador en el dataset. Para eso, mape\u00e9 cada indicador a una tupla (indicador, 1) y luego apliqu\u00e9 reduceByKey para sumar las ocurrencias. Posteriormente, orden\u00e9 los resultados en orden descendente de frecuencia usando sortBy.\n\nUna vez obtenidas las frecuencias, convert\u00ed el RDD resultante de vuelta a un DataFrame (indicatorCountsDF) usando toDF, lo cual me permiti\u00f3 visualizar los resultados de forma m\u00e1s c\u00f3moda con .show(). As\u00ed pude identificar f\u00e1cilmente los indicadores educativos m\u00e1s frecuentes en los datos del Banco Mundial.\n\nAdem\u00e1s, repet\u00ed exactamente el mismo procedimiento pero aplicado a los pa\u00edses. Seleccion\u00e9 la columna country_name, la convert\u00ed en un RDD de strings, cont\u00e9 las ocurrencias por pa\u00eds, orden\u00e9 los resultados, y los transform\u00e9 en un nuevo DataFrame (countryCountsDF). Esto me dio una vista clara de cu\u00e1les pa\u00edses tienen m\u00e1s registros educativos en el conjunto de datos, lo que puede indicar una mejor cobertura o una mayor disponibilidad hist\u00f3rica de datos."}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------------------------------------------------------------------------+-----+\n|indicator_name                                                                   |count|\n+---------------------------------------------------------------------------------+-----+\n|Population of the official entrance age to primary education, both sexes (number)|13   |\n|GNI (current US$)                                                                |10   |\n|Mortality rate, under-5 (per 1,000)                                              |10   |\n|Theoretical duration of upper secondary education (years)                        |10   |\n|Theoretical duration of primary education (years)                                |9    |\n|Gross enrolment ratio, primary and secondary, female (%)                         |9    |\n|Drop-out rate from Grade 5 of primary education, female (%)                      |8    |\n|Enrolment in secondary general, both sexes (number)                              |8    |\n|Gross enrolment ratio, primary, male (%)                                         |8    |\n|Population, total                                                                |8    |\n+---------------------------------------------------------------------------------+-----+\nonly showing top 10 rows\n\n"}, {"data": {"text/plain": "indicatorsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[74] at map at <console>:35\nindicatorCountsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[79] at sortBy at <console>:40\nindicatorCountsDF: org.apache.spark.sql.DataFrame = [indicator_name: string, count: int]\n"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "val indicatorsRDD = worldBankDF\n  .select(\"indicator_name\")\n  .rdd\n  .map(row => row.getString(0))\n\nval indicatorCountsRDD = indicatorsRDD\n  .map(indicator => (indicator, 1))\n  .reduceByKey(_ + _)\n  .sortBy(_._2, ascending = false)\n\nval indicatorCountsDF = indicatorCountsRDD.toDF(\"indicator_name\", \"count\")\nindicatorCountsDF.show(10, false)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"data": {"text/plain": "ventasDF: org.apache.spark.sql.DataFrame = [id_venta: int, id_producto: int ... 2 more fields]\n"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "val ventasDF = spark.read\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .csv(\"gs://scala-spark-datos/ventas.csv\")"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id_venta: integer (nullable = true)\n |-- id_producto: integer (nullable = true)\n |-- cantidad: integer (nullable = true)\n |-- precio_unitario: double (nullable = true)\n\n+--------+-----------+--------+---------------+\n|id_venta|id_producto|cantidad|precio_unitario|\n+--------+-----------+--------+---------------+\n|       1|        101|       5|           20.0|\n|       2|        102|       3|           15.0|\n|       3|        101|       2|           20.0|\n|       4|        103|       7|           10.0|\n|       5|        102|       4|           15.0|\n+--------+-----------+--------+---------------+\nonly showing top 5 rows\n\n"}], "source": "ventasDF.printSchema()\nventasDF.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"data": {"text/plain": "import org.apache.spark.sql.functions._\nventasConIngresoDF: org.apache.spark.sql.DataFrame = [id_venta: int, id_producto: int ... 3 more fields]\n"}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": "import org.apache.spark.sql.functions._\n\nval ventasConIngresoDF = ventasDF\n  .withColumn(\"ingreso_total\", col(\"cantidad\") * col(\"precio_unitario\"))"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+-----------+--------+---------------+-------------+\n|id_venta|id_producto|cantidad|precio_unitario|ingreso_total|\n+--------+-----------+--------+---------------+-------------+\n|       1|        101|       5|           20.0|        100.0|\n|       2|        102|       3|           15.0|         45.0|\n|       3|        101|       2|           20.0|         40.0|\n|       4|        103|       7|           10.0|         70.0|\n|       5|        102|       4|           15.0|         60.0|\n+--------+-----------+--------+---------------+-------------+\nonly showing top 5 rows\n\n"}], "source": "ventasConIngresoDF.show(5)"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/plain": "ingresosPorProductoDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_producto: int, ingreso_total: double ... 3 more fields]\n"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "val ingresosPorProductoDF = ventasConIngresoDF\n  .groupBy(\"id_producto\")\n  .agg(\n    sum(\"ingreso_total\").alias(\"ingreso_total\"),\n    sum(\"cantidad\").alias(\"cantidad_total\"),\n    avg(\"precio_unitario\").alias(\"precio_promedio\"),\n    count(\"id_venta\").alias(\"num_ventas\")\n  )\n  .orderBy(desc(\"ingreso_total\"))"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------------+--------------+---------------+----------+\n|id_producto|ingreso_total|cantidad_total|precio_promedio|num_ventas|\n+-----------+-------------+--------------+---------------+----------+\n|        104|        800.0|            16|           50.0|         5|\n|        105|        570.0|            19|           30.0|         5|\n|        109|        540.0|            20|           27.0|         4|\n|        110|        494.0|            26|           19.0|         4|\n|        108|        486.0|            27|           18.0|         5|\n|        101|        460.0|            23|           20.0|         6|\n|        106|        425.0|            17|           25.0|         5|\n|        102|        405.0|            27|           15.0|         6|\n|        107|        396.0|            18|           22.0|         5|\n|        103|        280.0|            28|           10.0|         5|\n+-----------+-------------+--------------+---------------+----------+\n\n"}], "source": "ingresosPorProductoDF.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "spylon-kernel", "language": "scala", "name": "spylon-kernel"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "help_links": [{"text": "MetaKernel Magics", "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"}], "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "0.4.1"}}, "nbformat": 4, "nbformat_minor": 4}